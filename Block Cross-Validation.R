# =============================================================================
# Spatial Cross-Validation Analysis in R
# Author: Vanesa Bejarano Alegre
# Date: 2025-02-22
# Email: vanesa.bejarano@gmail.com / van@vanwildlife.com
# Description: This script performs spatial cross-validation using the blockCV 
#              package and trains a GBM model to evaluate AUC performance.
# =============================================================================

# Spatial Cross-Validation --------------

# Load necessary libraries
library(blockCV) # For spatial cross-validation
library(sf) # For handling spatial objects
library(gbm) # For training the Gradient Boosting Machine (GBM) model
library(pROC) # For calculating the AUC

# 1. Convert the dataframe into a spatial object
# ----------------------------------------------
# The `datos_balanceados` dataset contains the coordinates 'X_' and 'Y_', which 
# represent the spatial location of the points.
# It is converted into a spatial object using the `st_as_sf()` function from the `sf` package.
# The spatial reference system EPSG:3395 (World Mercator) is used.

LECO_05KM_sf <- st_as_sf(datos_balanceados, coords = c("X_", "Y_"), crs = 3395)

# 2. Generate spatial cross-validation folds
# ----------------------------------------------
# Spatial blocks of 1000 meters in size are created and divided into 5 folds for cross-validation.
# The `cv_spatial()` function groups points into spatial blocks, ensuring that training 
# and testing data are spatially independent.

cv_blocks <- cv_spatial(x = LECO_05KM_sf, column = "case_", size = 1000, k = 5)

# Visualize the generated spatial blocks
cv_plot(cv_blocks)

# 3. Set up folds and evaluate the model in each cross-validation iteration
# -------------------------------------------------------------------------
# An empty list `results` is created to store AUC values for each fold.

results <- list()

# A `for` loop iterates over the 5 folds generated by `cv_blocks`
for(i in seq_along(cv_blocks$folds_list)) {
  
  # Extract training and testing indices for the current fold
  train_indices <- unlist(cv_blocks$folds_list[-i]) # Training data (all but the current fold)
  test_indices <- unlist(cv_blocks$folds_list[i]) # Testing data (current fold)
  
  # Split the dataset into training and testing sets
  train_data <- datos_balanceados[train_indices, ]
  test_data <- datos_balanceados[test_indices, ]
  
  # 4. Define sample weights for the model
  # ----------------------------------------------
  # Weights are assigned to the training data to balance classification.
  # If `case_ == 1`, a weight of 2 is assigned; if `case_ == 0`, a weight of 1 is assigned.
  
  train_weights <- ifelse(train_data$case_ == 1, 2, 1)
  
  # 5. Train the GBM model on the training data
  # ----------------------------------------------
  # A Gradient Boosting Machine (GBM) model is trained with a Bernoulli distribution (binary classification).
  # The model is fitted using 300 trees.
  
  mod_cv <- gbm(case_ ~ for_bin + nfn_inside + nfn_outside + 
                  pas_bin + agr_inside + agr_outside, 
                data = train_data, 
                distribution = "bernoulli", 
                n.trees = 300,
                weights = train_weights)
  
  # 6. Make predictions on the test data
  # ----------------------------------------------
  # Probabilities are predicted for the test dataset using the trained model.
  
  prob_predictions <- predict(mod_cv, newdata = test_data, type = "response")
  
  # 7. Compute the AUC (Area Under the ROC Curve)
  # ----------------------------------------------
  # The `pROC::auc()` function calculates the AUC value based on actual values and predictions.
  
  auc_score <- pROC::auc(test_data$case_, prob_predictions)
  
  # 8. Store the AUC result in the `results` list
  results[[i]] <- auc_score
}

# 9. Display the results of spatial cross-validation
# ---------------------------------------------------
# Print the `results` list, which contains the AUC values obtained for each fold.
print(results)

# AUC Interpretation:
# 0.5 → Model has no predictive power (random).
# 0.6 - 0.7 → Poor model (low discrimination ability).
# 0.7 - 0.8 → Acceptable model.
# 0.8 - 0.9 → Good model.
# 0.9 - 1.0 → Excellent model. # However, it is advisable to check for overfitting.


